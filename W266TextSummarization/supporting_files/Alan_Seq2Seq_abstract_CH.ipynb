{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/huyue012/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/huyue012/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/huyue012/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huyue012/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from cytoolz import concatv\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk,pprint\n",
    "from nltk import word_tokenize\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize_rm_punct (blob):\n",
    "    bexp = ''    \n",
    "    blob.replace('\\n','')\n",
    "    #blob.replace('</p>','  ')\n",
    "    #print(type(blob))\n",
    "\n",
    "    for i, char in (enumerate(blob)):\n",
    "        #print(i,char)\n",
    "        next_cap = False\n",
    "        prev_lower = False\n",
    "        next_space = False\n",
    "        \n",
    "        if char in string.punctuation :\n",
    "            #print(\"found punctuation:\", i, char, blob[i+1])\n",
    "           \n",
    "            if char in '.?!' :\n",
    "                if i+1 >= len(blob):\n",
    "                    next_cap = False\n",
    "                else:\n",
    "                    next_cap = blob[i+1].isupper()\n",
    "                    next_space = blob[i+1].isspace()                    \n",
    "\n",
    "                if i-1 <0:\n",
    "                    prev_lower = False\n",
    "                else:\n",
    "                    prev_lower = blob[i-1].islower()\n",
    "\n",
    "                if (next_cap and prev_lower) or next_space: \n",
    "                    # if the char before \".\" is lower case, but the one immediately follow the \".\" is Uppercase, then this is a paragraph end (caused by removing <p></p> from the xml file)\n",
    "                    bexp = bexp + ' </s> '\n",
    "                else:\n",
    "                    #if this is not end of paragraph, then \n",
    "                    #bexp = bexp + char\n",
    "                    pass\n",
    "            else :\n",
    "                bexp = bexp + ' '\n",
    "        elif char == '\\n':\n",
    "            i += 1                    \n",
    "        else :\n",
    "            if char.isnumeric() : \n",
    "                bexp = bexp + 'N'\n",
    "            else :\n",
    "                bexp = bexp + char.lower()\n",
    "    \n",
    "    #return(sent_tokenize(bexp))\n",
    "    return(bexp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/huyue012/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    " \n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # top N vocab?\n",
    "# def create_lookup_tables(text, V=10000):\n",
    "#     # make a list of unique words\n",
    "#     CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 , '<p>': 4}\n",
    "#     print(\"length of create_lookup_table input:\", len(text))\n",
    "#     print(\"type of create_lookup_table input:\", type(text))\n",
    "    \n",
    "#     words = text.split() #??\n",
    "#     unigram_counts = Counter()\n",
    "#     prev_word = None\n",
    "#     for w in words:\n",
    "#         unigram_counts[w] += 1\n",
    "#         prev_word = word\n",
    "\n",
    "#     # Leave space for codes above\n",
    "#     top_counts = unigram_counts.most_common(V - len(CODES))\n",
    "\n",
    "# #     vocab = set(words)\n",
    "#     vocab = [w for w,c in top_counts]\n",
    "\n",
    "#     # (1)\n",
    "#     # starts with the special tokens\n",
    "#     vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "#     # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
    "#     # since vocab_to_int already contains special tokens\n",
    "#     for v_i, v in enumerate(vocab, len(CODES)):\n",
    "#         vocab_to_int[v] = v_i\n",
    "#         #print(v_i, v)\n",
    "#     # (2)\n",
    "#     int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "# #     print(len(vocab_to_ind), vocab_to_int[:6])\n",
    "    \n",
    "#     return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    # make a list of unique words\n",
    "    CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 , '<p>': 4}\n",
    "    print(\"length of create_lookup_table input:\", len(text))\n",
    "    print(\"type of create_lookup_table input:\", type(text))\n",
    "    \n",
    "    vocab = set(text.split())\n",
    "#     vocab = set(''.join(list(body_text)).split() + text.split())\n",
    "    \n",
    "    # remove stop words\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    vocab_filtered = [w for w in vocab if w not in stopWords]\n",
    "    \n",
    "    # (1)\n",
    "    # starts with the special tokens\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
    "    # since vocab_to_int already contains special tokens\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "        #print(v_i, v)\n",
    "    # (2)\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "        1st, 2nd args: raw string text to be converted\n",
    "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
    "    \n",
    "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
    "    \"\"\"\n",
    "    # empty list of converted sentences\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    #source_sentences = source_text.split(\"\\n\")\n",
    "    #target_sentences = target_text.split(\"\\n\")\n",
    "    source_sentences = source_text\n",
    "    target_sentences = target_text\n",
    "\n",
    "    \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "    \n",
    "    # iterating through each sentences (# of sentences in source&target is the same)\n",
    "    for i in range(len(source_sentences)):\n",
    "        # extract sentences one by one\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split(\" \")\n",
    "        target_tokens = target_sentence.split(\" \")\n",
    "        \n",
    "        # empty list of converted words to index in the chosen sentence\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if (token != \"\"):\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if (token != \"\"):\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target sentence\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted sentences in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def preprocess_and_save_data(source_text, target_text, text_to_ids):\n",
    "    # Preprocess\n",
    "    \n",
    "\n",
    "    # create lookup tables for English and French data\n",
    "    #source_text is a Pandas dataframe series, so is target_text\n",
    "    #to create lookup_tables, we will join all the lines together and send as text. \n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(''.join(list(source_text)))\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(''.join(list(target_text)))\n",
    "    \n",
    "\n",
    "    # create list of sentences whose words are represented in index\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "    \n",
    "    print(type(source_text), len(source_text))\n",
    "    print(source_text[10:12])\n",
    "    # Save data for later use\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_preprocess():\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoint for procedures used to perform pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> 0\n",
      "/1988/03/23/0129960.xml     new york city s ambitious effort to give physi...\n",
      "/1988/03/23/0129961.xml     after revealing on national television that he...\n",
      "/1988/03/23/0129962.xml     while standing  smooth down as much fabric as ...\n",
      "/1996/06/30/0861580.xml     an imaginative chef can spice up assorted left...\n",
      "/1996/06/30/0861588.xml     the nutty professoreddie murphy  jada pinkett ...\n",
      "Name: 3, dtype: object\n",
      "<class 'pandas.core.series.Series'> 0\n",
      "/1988/03/23/0129960.xml      new york city dept of health and mental hygie...\n",
      "/1988/03/23/0129961.xml      gov james e mcgreevey  whose insistence on st...\n",
      "/1988/03/23/0129962.xml      women wearing miniskirts describe how to sit ...\n",
      "/1996/06/30/0861580.xml      anna kisselgoff reviews american ballet theat...\n",
      "/1996/06/30/0861588.xml      laurel graeber reviews the nutty professor as...\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "#parameter setting\n",
    "work_dir = \"working_dir/\"\n",
    "#read file into Pandas Dataframe.\n",
    "#each line is one record, it contains \"original file name(including y/m/d)\",\"TITLE\",\"ABSTRACT\",\"LEAD_PARAGRAPH\",\"FULL_TEXT\"\n",
    "nyt_data = pd.read_csv('abstract_nyt_structured_data_1000.csv', \n",
    "                       delimiter=',', index_col=0, header=None, quotechar='\"', quoting=1, \n",
    "                       skipinitialspace=True, engine='c')\n",
    "\n",
    "#We will use LEAD_PARAGRAPH as source for now\n",
    "#And ABSTRACT as target. \n",
    "# Remove punctuation, change digits to NNN, and to lower case.     \n",
    "source_text=nyt_data[3].apply(sent_tokenize_rm_punct)\n",
    "target_text=nyt_data[2].apply(sent_tokenize_rm_punct)\n",
    "body_text = nyt_data[4].apply(sent_tokenize_rm_punct)\n",
    "print(type(source_text),source_text[0:5])\n",
    "print(type(target_text),target_text[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/1988/03/23/0129960.xml</th>\n",
       "      <td>For Health Survey, Many Offer More Excuses Tha...</td>\n",
       "      <td>New York City Dept of Health and Mental Hygie...</td>\n",
       "      <td>New York City's ambitious effort to give physi...</td>\n",
       "      <td>New York City's ambitious effort to give physi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/1988/03/23/0129961.xml</th>\n",
       "      <td>McGreevey Seems Set To Exit On His Terms</td>\n",
       "      <td>Gov James E McGreevey, whose insistence on st...</td>\n",
       "      <td>After revealing on national television that he...</td>\n",
       "      <td>After revealing on national television that he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          1  \\\n",
       "0                                                                             \n",
       "/1988/03/23/0129960.xml   For Health Survey, Many Offer More Excuses Tha...   \n",
       "/1988/03/23/0129961.xml           McGreevey Seems Set To Exit On His Terms    \n",
       "\n",
       "                                                                          2  \\\n",
       "0                                                                             \n",
       "/1988/03/23/0129960.xml    New York City Dept of Health and Mental Hygie...   \n",
       "/1988/03/23/0129961.xml    Gov James E McGreevey, whose insistence on st...   \n",
       "\n",
       "                                                                          3  \\\n",
       "0                                                                             \n",
       "/1988/03/23/0129960.xml   New York City's ambitious effort to give physi...   \n",
       "/1988/03/23/0129961.xml   After revealing on national television that he...   \n",
       "\n",
       "                                                                          4  \n",
       "0                                                                            \n",
       "/1988/03/23/0129960.xml   New York City's ambitious effort to give physi...  \n",
       "/1988/03/23/0129961.xml   After revealing on national television that he...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc, head, abstract, lead paragraph, full\n",
    "nyt_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After revealing on national television that he is gay, seeing his name become the punch line of countless late-night comedy gags, and fending off a coup attempt from within his own party, Gov. James E. McGreevey needs only to remain in office until midnight Friday to outlast the state deadline that would automatically force a special election to choose his successor.Mr. McGreevey announced on Aug. 12 that he was stepping down because he had had an extramarital affair with a man he did not identify, but his insistence on remaining in office until Nov. 15 set off political, legal and public relations challenges. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'after revealing on national television that he is gay  seeing his name become the punch line of countless late night comedy gags  and fending off a coup attempt from within his own party  gov </s>  james e </s>  mcgreevey needs only to remain in office until midnight friday to outlast the state deadline that would automatically force a special election to choose his successor </s> mr </s>  mcgreevey announced on aug </s>  NN that he was stepping down because he had had an extramarital affair with a man he did not identify  but his insistence on remaining in office until nov </s>  NN set off political  legal and public relations challenges </s>  '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nyt_data[3][1]\n",
    "print(a)\n",
    "sent_tokenize_rm_punct(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(len(nyt_data),len(source_text))\n",
    "work_dir = \"working_dir/\"\n",
    "work_dir2 = \"working_dir2/\"\n",
    "# save_path = work_dir2\n",
    "\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of create_lookup_table input: 724292\n",
      "type of create_lookup_table input: <class 'str'>\n",
      "length of create_lookup_table input: 221117\n",
      "type of create_lookup_table input: <class 'str'>\n",
      "<class 'list'> 1000\n",
      "[[11732, 6583, 481, 1337, 13167, 9850, 3401, 247, 14473, 13654, 13066, 6871, 10166, 14473, 2240, 14452, 2030, 3175, 10166, 7633, 11514, 9850, 5869, 1934, 2905, 14136, 3790, 11920, 10166, 6394, 136, 6583, 573, 8323, 6305, 14239, 12184, 1532, 7267, 14703, 8519, 1022, 14435, 3028, 11514, 12813, 13534, 2119, 8094, 13087, 630, 1495, 13653, 14985, 13534], [320, 6642, 5023, 3153, 2772, 8925, 11732, 5841, 13534, 12083, 3283, 3071, 10127, 2913, 13066, 6583, 6891, 501, 5822, 3028, 5678, 957, 1003, 8018, 2199, 14498, 15442, 11338, 4547, 12660, 14136, 12668, 1910, 12668, 10557, 13117, 327, 10166, 6583, 8653, 9380, 9569, 680, 5841, 10166, 9678, 11206, 13534]]\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save_data(source_text, target_text, text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a training process in decoding layer \n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a inference process in decoding layer \n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 10\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 50\n",
    "\n",
    "rnn_size = 200\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.01\n",
    "keep_probability = 0.5\n",
    "save_path = work_dir\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "\n",
    "# max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "max_target_sentence_length = min(150,max([len(sentence) for sentence in source_int_text]))\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    #lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "\n",
    "    lr = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "  \n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(input_data, #tf.reverse(input_data, [-1])\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    " \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "#     max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    max_sentence = min(150, max([len(sentence) for sentence in sentence_batch]))\n",
    "    return [sentence[:150] + [pad_int] * (max_sentence - len(sentence[:150])) for sentence in sentence_batch]\n",
    "#     return [sentence for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n",
    "        \n",
    "        \n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                 valid_target,\n",
    "                 batch_size,\n",
    "                 source_vocab_to_int['<PAD>'],\n",
    "                 target_vocab_to_int['<PAD>']))                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   10/20 - Train Accuracy: 0.6600, Validation Accuracy: 0.7028, Loss: 3.3658\n",
      "Epoch   1 Batch   10/20 - Train Accuracy: 0.6602, Validation Accuracy: 0.7029, Loss: 3.0010\n",
      "Epoch   2 Batch   10/20 - Train Accuracy: 0.6605, Validation Accuracy: 0.7035, Loss: 3.0845\n",
      "Epoch   3 Batch   10/20 - Train Accuracy: 0.6627, Validation Accuracy: 0.7033, Loss: 2.9782\n",
      "Epoch   4 Batch   10/20 - Train Accuracy: 0.6632, Validation Accuracy: 0.7044, Loss: 2.9363\n",
      "Epoch   5 Batch   10/20 - Train Accuracy: 0.6636, Validation Accuracy: 0.7048, Loss: 2.9026\n",
      "Epoch   6 Batch   10/20 - Train Accuracy: 0.6634, Validation Accuracy: 0.7051, Loss: 2.8894\n",
      "Epoch   7 Batch   10/20 - Train Accuracy: 0.6634, Validation Accuracy: 0.7039, Loss: 2.7879\n",
      "Epoch   8 Batch   10/20 - Train Accuracy: 0.6632, Validation Accuracy: 0.7057, Loss: 2.7778\n",
      "Epoch   9 Batch   10/20 - Train Accuracy: 0.6657, Validation Accuracy: 0.7047, Loss: 2.7145\n",
      "Epoch  10 Batch   10/20 - Train Accuracy: 0.6641, Validation Accuracy: 0.7043, Loss: 2.6533\n",
      "Epoch  11 Batch   10/20 - Train Accuracy: 0.6666, Validation Accuracy: 0.7052, Loss: 2.7164\n",
      "Epoch  12 Batch   10/20 - Train Accuracy: 0.6661, Validation Accuracy: 0.7057, Loss: 2.6002\n",
      "Epoch  13 Batch   10/20 - Train Accuracy: 0.6686, Validation Accuracy: 0.7039, Loss: 2.5464\n",
      "Epoch  14 Batch   10/20 - Train Accuracy: 0.6666, Validation Accuracy: 0.7053, Loss: 2.8063\n",
      "Epoch  15 Batch   10/20 - Train Accuracy: 0.6684, Validation Accuracy: 0.7047, Loss: 2.5126\n",
      "Epoch  16 Batch   10/20 - Train Accuracy: 0.6702, Validation Accuracy: 0.7049, Loss: 2.4399\n",
      "Epoch  17 Batch   10/20 - Train Accuracy: 0.6714, Validation Accuracy: 0.7057, Loss: 2.4306\n",
      "Epoch  18 Batch   10/20 - Train Accuracy: 0.6720, Validation Accuracy: 0.7023, Loss: 2.5288\n",
      "Epoch  19 Batch   10/20 - Train Accuracy: 0.6736, Validation Accuracy: 0.6960, Loss: 2.4071\n",
      "Epoch  20 Batch   10/20 - Train Accuracy: 0.6709, Validation Accuracy: 0.7052, Loss: 2.4140\n",
      "Epoch  21 Batch   10/20 - Train Accuracy: 0.6698, Validation Accuracy: 0.7065, Loss: 2.3414\n",
      "Epoch  22 Batch   10/20 - Train Accuracy: 0.6741, Validation Accuracy: 0.6992, Loss: 2.3648\n",
      "Epoch  23 Batch   10/20 - Train Accuracy: 0.6750, Validation Accuracy: 0.6831, Loss: 2.3406\n",
      "Epoch  24 Batch   10/20 - Train Accuracy: 0.6770, Validation Accuracy: 0.6832, Loss: 2.2874\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 950\n",
      "[[9245, 7334, 23363, 9934, 11280, 22635, 29948, 28905, 30380, 32974, 28803, 5473, 28589, 29948, 27880, 17723, 26196, 20130, 27517, 15987, 22843, 6366, 31669, 15395, 7935, 29097, 29948, 10882, 32431, 13127, 15442, 35895, 12648, 23363, 4238, 35602, 28410, 29948, 18593, 3467, 33243, 29948, 2589, 25364, 5208, 13125, 12164, 3467, 23363, 9934, 4238, 36253, 7077, 33964, 7935, 25364, 11106, 25214, 1759, 36709, 24181, 12648, 31426, 24129, 6366, 3467, 22635, 18758, 30208, 13845, 7077, 9245, 19426, 3624, 13127, 5943, 17283, 31269, 30907, 4906, 12173, 4906, 23155, 28803, 10933, 4906, 20877, 29948, 3739, 12648], [34846, 2835, 1383, 36415, 20045, 7077, 8806, 10286, 1093, 23971, 1010, 32747, 3319, 3467, 12124, 3186, 12164, 25592, 31146, 28791, 10180, 9424, 28803, 30880, 17283, 25364, 3557, 18554, 1759, 36707, 1010, 16315, 21153, 30855, 12648, 35543, 12199, 12648, 18040, 9842, 18365, 29948, 8103, 13127, 17576, 26052, 15942, 28004, 29948, 36330, 3467, 35130, 27795, 7077, 7657, 11584, 8678, 25364, 3311, 21824, 29948, 33498, 1010, 34861, 12648, 9858, 12648, 18040, 34783, 1383, 33323, 12648, 26250, 7077, 8806, 34505, 26146, 4463, 21960, 8806, 15827, 15827, 4924, 23245, 11641, 11645, 25364, 22256, 8806, 16074, 3663, 22581, 36253, 1010, 5562, 1383, 21174, 13127, 17576, 26052, 13262, 12648, 26250, 20222, 17283, 26195, 25207, 28803, 25615, 29470, 26563, 12648]]\n"
     ]
    }
   ],
   "source": [
    "# train_source = source_int_text[batch_size:] \n",
    "print(len(source_int_text), len(train_source))\n",
    "print(source_int_text[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5254,  8036, 14555, 12764,  6366,  7589,  4914, 13653,  6798,\n",
       "         8726,  5355,  5213,  4815,  4914, 12889,  8771, 15373,  1718,\n",
       "        14997,  2770, 12947,  6483, 11585,  6119, 10662,   373,  4914,\n",
       "          451,  3907,    46,  7382, 13938,   885, 14555, 10138, 10245,\n",
       "        15231,  4914,  1833,  3710,  3543,  4914, 11904,  4028, 13370,\n",
       "        12347,  3658,  3710, 14555, 12764, 10138,  6205,  7871, 13084,\n",
       "        10662,  4028,  5004, 12933,  2954,  8870, 14413,   885,  8849,\n",
       "        15431,  6483,  3710,  7589,  9514, 13098,  1925,  7871,  5254,\n",
       "         6002,  4775,    46,  7320,  5857, 13282,   945, 15257,  9548,\n",
       "        15257,  7230,  5355, 14629, 15257,    56,  4914,  9411,   885,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [ 3192, 10257, 13340,  7387,  3638,  7871,  7449,  8632,  4865,\n",
       "         8809,  3841, 11926,  9976,  3710,  3660,  1879,  3658,  8485,\n",
       "         2304, 14564,  1381,  4811,  5355, 10394,  5857,  4028, 15414,\n",
       "        15531,  2954, 14185,  3841,  7259, 13791, 12087,   885, 13591,\n",
       "         2416,   885,  3960, 13553, 12949,  4914,  6903,    46, 11423,\n",
       "         2674,  4222,  1316,  4914, 12558,  3710,  1743,  7375,  7871,\n",
       "        14584, 10550, 14240,  4028,  8226, 13903,  4914, 15111,  3841,\n",
       "         2956,   885, 14644,   885,  3960,  8191, 13340,   121,   885,\n",
       "         9099,  7871,  7449,   520,  2087,   333,  8598,  7449, 10437,\n",
       "        10437,  2221, 13605,  9646,  8999,  4028,  4584,  7449, 14040,\n",
       "         3649,  6400,  6205,  3841,  7147, 13340,  5313,    46, 11423,\n",
       "         2674,  3851,   885,  9099, 12507,  5857,  7751, 12590,  5355,\n",
       "         4834, 11162,  5639,   885,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(valid_sources_batch))\n",
    "valid_sources_batch[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from working_dir/\n",
      "Input\n",
      "  Word Ids:      [2, 2, 5152, 15397, 12668, 1233, 7052, 12668, 4900, 7751, 10657, 2, 2, 11390, 6583, 2, 9763, 10166, 2, 2, 2, 9452, 13712, 10114, 2, 15053, 2724, 9380, 6583, 5664, 1933, 3028, 15254, 10574, 8314, 2, 4578, 11359, 8314, 2, 12668, 5394, 6968, 11732, 7835, 3071, 3158, 2913, 7751, 3039, 6583, 2267, 10166, 6515, 5394, 320, 2, 2]\n",
      "  English Words: ['<UNK>', '<UNK>', 'smooth', 'down', 'as', 'much', 'fabric', 'as', 'possible', 'with', 'both', '<UNK>', '<UNK>', 'cross', 'the', '<UNK>', 'bend', 'and', '<UNK>', '<UNK>', '<UNK>', 'place', 'purse', 'over', '<UNK>', 'peculiar', 'choreography', 'is', 'the', 'daily', 'routine', 'for', 'women', 'wearing', 'miniskirts', '<UNK>', 'extremely', 'short', 'miniskirts', '<UNK>', 'as', 'they', 'try', 'to', 'grab', 'a', 'subway', 'seat', 'with', 'all', 'the', 'grace', 'and', 'modesty', 'they', 'can', '<UNK>', '<UNK>']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [1835, 4919, 5283, 2347, 499, 4919, 7664, 1]\n",
      "  French Words: new of reviews book on of s <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "\n",
    "ex0 = 'he saw a old yellow truck .'\n",
    "ex1 = nyt_data[3][2]\n",
    "ex2 = ' '.join([source_int_to_vocab[i] for i in source_int_text[1]])\n",
    "translate_sentence = ex1\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_path + '.meta')\n",
    "    loader.restore(sess, save_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  While standing, smooth down as much fabric as possible with both hands. Then cross the knees, bend and sit. Optional precaution: place purse over lap.This peculiar choreography is the daily routine for women wearing miniskirts -- extremely short miniskirts -- as they try to grab a subway seat with all the grace and modesty they can muster. \n",
      "\n",
      "Target:   Women wearing miniskirts describe how to sit on subway seat with modicum of grace and modesty; photos (M)  \n",
      "\n",
      "Prediction:  new york reviews to sellers of new and long and long and long and long and long and long and NNN and long and NNN and long and NNN and long and NNN and long and NNN and NNN and NNN and NNN and NNN and NNN and NNN and NNN and NNN and NNN <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(\"Source: \",nyt_data[3][2])\n",
    "print(\"\\nTarget: \",nyt_data[2][2])\n",
    "print(\"\\nPrediction: \", \" \".join([target_int_to_vocab[i] for i in translate_logits]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full article:  While standing, smooth down as much fabric as possible with both hands. Then cross the knees, bend and sit. Optional precaution: place purse over lap.This peculiar choreography is the daily routine for women wearing miniskirts -- extremely short miniskirts -- as they try to grab a subway seat with all the grace and modesty they can muster.The city can often be an obstacle course for fashionistas, with grates that threaten to trip stilettos at every corner and humidity that can turn any fabric into virtual Saran Wrap.But the chic cannot be ruled by fear. So when hemlines that fall just centimeters below one's bottom became all the rage this summer, young women who dared found a way.The goal is to avoid having your bottom ''touch too much of the seat,'' said Jessica Oser, 24, who sported a lavender polo shirt and khaki mini as she browsed at the Union Square farmers' market this week.Ms. Oser, a student at the Benjamin N. Cardozo School of Law, never wears a miniskirt when she takes a New Jersey Transit train to visit her parents, since crossing her legs properly seems infinitely more difficult on those train seats.The consensus is that not all miniskirts are created equal. Pleated fabric is better because it moves around with the derri&#xE8;re; denim is stiff and more likely to get in the way; cotton is neater to smooth out. Then again, cotton can also ride up. All things considered, corduroy may be the best solution for the mini-wary.''There's something about them that's right on the edge of stepping over the line,'' said Lia Aprile, a 23-year-old actress who lives in Greenwich Village. ''But they're comfortable, you can bend over and sit down.''Ms. Aprile was late to pick up the trend. Her first miniskirt purchase was only last month, after a friend gave her a gift certificate to Bloomingdale's.She scanned the aisles for skirts, but all she could find were the shortest kind. ''They were the only things on the rack,'' she explained, with just a touch of embarrassment. Now, she owns two and is quickly becoming nonchalant about the whole process.''I don't mind if I miss and don't manage to get all the skirt down,'' she said. ''Any skin on the plastic means more cool on the back of your legs.''Heather Marks, a 16-year-old model from Canada who travels between New York City and Calgary, says minis are more popular in New York, although they require more maneuvering, because New Yorkers tend to get around by foot or subway, rather than in cars. ''You always have to be adjusting,'' she said, tugging at her cotton black flower print miniskirt as she waited at Union Square for the subway. ''You have to make sure to bend your knees the right way as soon as you sit down.''Once on board, she made one smooth motion.''You have to duck into it,'' she explained.But beware -- dangers are not limited to below ground. Those subway grates can blow up miniskirts the same way they got Marilyn Monroe's dress. One unsuspecting tourist from Massachusetts exposed more than she realized while walking in Times Square on Wednesday afternoon.Which brings us to another rule: stay away from any fabric that might be called flowing. It is far more likely to stick to the seats or be caught in a compromising position, miniskirt fans say.While more than a dozen women interviewed said they had never spoken about their subway miniskirt etiquette with friends, they were effusive in their explanations to this reporter, motioning and bending in every direction for the sake of clarity. (Only one subject was off limits. Choice undergarments for skirts that bare skin? No comment.)Nicole Leone, 31, usually has her 4-month-old son in a stroller with her when she travels by subway, so she has learned the art of the one-handed tug-and-sit.''Of course it's better if you have two hands, but what else am I supposed to do?'' she asked. ''It's always easier if my husband is there.''Her husband's presence also may curtail any unwelcome looks Ms. Leone's fashion choices may attract.''Sometimes, you'll have forgotten to be extra careful and you'll see this certain type of guy looking too closely at your knees,'' she said. ''Then you have to pinch your legs together real tight and stay that way the rest of the ride.''To avoid such problems, Ms. Oser, the law student, has learned that it is easier if she slouches on the subway, but such a posture does little for one's sex appeal.The real solution, her roommate says, is to take a cab.\n"
     ]
    }
   ],
   "source": [
    "print(\"Full article: \", nyt_data[4][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'after revealing on national television that he is gay seeing his name become the punch line of countless late night comedy gags and fending off a coup attempt from within his own party gov </s> james e </s> mcgreevey needs only to remain in office until midnight friday to outlast the state deadline that would automatically force a special election to choose his successor </s> mr </s> mcgreevey announced on aug </s> NN that he was stepping down because he had had an extramarital affair with a man he did not identify but his insistence on remaining in office until nov </s> NN set off political legal and public relations challenges </s>'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([source_int_to_vocab[i] for i in source_int_text[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at line 66036",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ac6008ef0f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m nyt_data = pd.read_csv('data/abstract_nyt_structured_data.csv', \n\u001b[1;32m      2\u001b[0m                        \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                        skipinitialspace=True, engine='c')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at line 66036"
     ]
    }
   ],
   "source": [
    "nyt_data = pd.read_csv('data/abstract_nyt_structured_data.csv', \n",
    "                       delimiter=',', index_col=0, header=None, quotechar='\"', quoting=1, \n",
    "                       skipinitialspace=True, engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
