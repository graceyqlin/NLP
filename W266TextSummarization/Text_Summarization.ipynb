{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import TextRanker\n",
    "\n",
    "import importlib\n",
    "importlib.reload(TextRanker)\n",
    "import evaluation_utils\n",
    "importlib.reload(evaluation_utils)\n",
    "import evaluation_utils as e\n",
    "\n",
    "from TextRanker import TextRanker\n",
    "\n",
    "import time\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load New York Times data into a pandas dataframe\n",
    "\n",
    "data4 = pd.read_csv('../112518_data.csv',delimiter=',', \n",
    "                    index_col=0, header=None, quotechar='\"', quoting=1, \n",
    "                    skipinitialspace=True, engine='c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title is The M Line and the Hemline: Miniskirt Protocols \n",
      "===============================================\n",
      "abstract is Women wearing miniskirts describe how to sit on subway seat with modicum of grace and modesty; photos (M) \n",
      "===============================================\n",
      "estimated summary is  This peculiar choreography is the daily routine for women wearing miniskirts-- extremely short miniskirts-- as they try to grab a subway seat with all the grace and modesty they can muster. The consensus is that not all miniskirts are created equal. You always have to be adjusting,'' she said, tugging at her cotton black flower print miniskirt as she waited at Union Square for the subway.'' Those subway grates can blow up miniskirts the same way they got Marilyn Monroe's dress. It is far more likely to stick to the seats or be caught in a compromising position, miniskirt fans say. Choice undergarments for skirts that bare skin?\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# Initial data exploration\n",
    "\n",
    "index = 2\n",
    "\n",
    "sample_title = data4.iloc[index,0]\n",
    "\n",
    "sample_abstract = data4.iloc[index,1]\n",
    "\n",
    "sample_lead = data4.iloc[index,2]\n",
    "\n",
    "sample_body = data4.iloc[index,3]\n",
    "\n",
    "print(\"title is\", sample_title)\n",
    "print(\"===============================================\")\n",
    "\n",
    "print(\"abstract is\", sample_abstract )\n",
    "print(\"===============================================\")\n",
    "\n",
    "\n",
    "TR = TextRanker()\n",
    "estimated_summary = TR.run(sample_body)\n",
    "\n",
    "sample_summary = estimated_summary\n",
    "\n",
    "print(\"estimated summary is \", estimated_summary)\n",
    "print(\"===============================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge_1/f_score': 0.15686274206651293,\n",
       " 'rouge_1/r_score': 0.42105263157894735,\n",
       " 'rouge_1/p_score': 0.0963855421686747,\n",
       " 'rouge_2/f_score': 0.047999997534720125,\n",
       " 'rouge_2/r_score': 0.16666666666666666,\n",
       " 'rouge_2/p_score': 0.028037383177570093,\n",
       " 'rouge_l/f_score': 0.06645896676914403,\n",
       " 'rouge_l/r_score': 0.3684210526315789,\n",
       " 'rouge_l/p_score': 0.06481481481481481}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate what's the rouge scores for this sample\n",
    "\n",
    "e.evaluate(sample_abstract, sample_summary, 'rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get outputs with the articles' abstracts and Text Ranker generated abstracts\n",
    "\n",
    "def get_estimate(start, end):\n",
    "    t0 = time.time()\n",
    "    total_abstract = []\n",
    "    total_summary = []\n",
    "    \n",
    "    for index in range(start, end):\n",
    "\n",
    "        sample_abstract = data4.iloc[index,1]\n",
    "        total_abstract.append(sample_abstract)\n",
    "\n",
    "        text = data4.iloc[index,3]\n",
    "        sample_summary = TR.run(text)\n",
    "        total_summary.append(sample_summary)\n",
    "    \n",
    "    db = pd.DataFrame({'abstract': total_abstract,'summary': total_summary})\n",
    "    db.to_csv(\"output/output_{}_{}.csv\".format(start, end), index = None)\n",
    "    print(\"finish job {} to {} in {}s\".format(start, end, round(time.time() - t0, 2)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing outputs\n",
    "\n",
    "N = data4.shape[0]\n",
    "batch_size = 500\n",
    "num = N // batch_size\n",
    "print(num)\n",
    "TR = TextRanker()\n",
    "for i in range(15, num+1):\n",
    "    get_estimate(i*batch_size, (i+1)*batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' note\n",
    "## table data\n",
    "index = 0: header\n",
    "index = 1: abstract  <-\n",
    "index = 2: lead para  <-\n",
    "index = 3: body\n",
    "\n",
    "## table output\n",
    "index = 0: abstract  <-\n",
    "index = 1: model-generated abstract  <-\n",
    "\n",
    "'''\n",
    "\n",
    "def my_eval(output, index_1 = 0, index_2 = 1):\n",
    "    r_1_r = 0\n",
    "    r_2_r = 0\n",
    "    r_l_r = 0\n",
    "    N = output.shape[0]\n",
    "    for i in range(0, N):\n",
    "        score_map = e.evaluate(output.iloc[i, index_1], output.iloc[i, index_2], 'rouge')\n",
    "        r_1_r += score_map['rouge_1/r_score']\n",
    "        r_2_r += score_map['rouge_2/r_score']\n",
    "        r_l_r += score_map['rouge_l/r_score']\n",
    "    return r_1_r / N * 100, r_2_r / N * 100, r_l_r / N * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file output/output_2000_2500.csv\n",
      "processing file output/output_6500_7000.csv\n",
      "processing file output/output_3500_4000.csv\n",
      "processing file output/output_8000_8500.csv\n",
      "processing file output/output_2500_3000.csv\n",
      "processing file output/output_9500_10000.csv\n",
      "processing file output/output_0_500.csv\n",
      "processing file output/output_1500_2000.csv\n",
      "processing file output/output_8500_9000.csv\n",
      "processing file output/output_1000_1500.csv\n",
      "processing file output/output_3000_3500.csv\n",
      "processing file output/output_10000_10500.csv\n",
      "processing file output/output_4000_4500.csv\n",
      "processing file output/output_500_1000.csv\n",
      "processing file output/output_5500_6000.csv\n",
      "processing file output/output_6000_6500.csv\n",
      "processing file output/output_9000_9500.csv\n",
      "processing file output/output_7500_8000.csv\n",
      "processing file output/output_7000_7500.csv\n",
      "processing file output/output_5000_5500.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rouge_1    38.058294\n",
       "rouge_2    17.525696\n",
       "rouge_l    31.141625\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate rouge scores for all outputs (the first 10,000 observations)\n",
    "\n",
    "fs = glob.glob(\"output/*.csv\")\n",
    "output_df = []\n",
    "for f in fs:\n",
    "    print(\"processing file {}\".format(f))\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        output_df.append(list(my_eval(df)))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "pd.DataFrame(output_df, columns = [ 'rouge_1', 'rouge_2', 'rouge_l' ] ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45.97678238352904, 21.96768489907332, 39.40375008549876)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate based line rouge scores for abstracts and lead paragraphs (the first 10,000 observations)\n",
    "\n",
    "my_eval(data4[:10000], index_1 = 1, index_2 = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
