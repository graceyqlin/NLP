{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of environemnt, imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/i812749/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/i812749/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import absolute_import\n",
    "#from __future__ import print_function\n",
    "#from __future__ import division\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import pandas as pd\n",
    "import json, os, re, shutil, sys, time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Queue\n",
    "from multiprocessing import Pool\n",
    "import collections, itertools\n",
    "import unittest\n",
    "import datetime\n",
    "import string\n",
    "import pickle\n",
    "import string\n",
    "import copy\n",
    "\n",
    "from cytoolz import concatv\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk,pprint\n",
    "from nltk import word_tokenize\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "#parameter setting\n",
    "work_dir = \"working_dir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize_rm_punct (blob):\n",
    "    #input is string\n",
    "    #remove punctuations\n",
    "    #change all to lower case\n",
    "    #change 0-9 to N (we keep the number of digits so 321 will be NNN this way system will try to generate NNN in the approximate magnitute)\n",
    "    #output is a list of strings (using ,?!, as senence spliter and split the sentence)\n",
    "    bexp=''\n",
    "    blob.replace('\\n','')\n",
    "    #blob.replace('</p>','  ')\n",
    "\n",
    "    for i, char in (enumerate(blob)):\n",
    "        #print(i,char)\n",
    "        next_cap = False\n",
    "        prev_lower = False\n",
    "        next_space = False\n",
    "        \n",
    "        if char in string.punctuation :\n",
    "            #print(\"found punctuation:\", i, char, blob[i+1])\n",
    "           \n",
    "            if char in '.?!' :\n",
    "                if i+1 >= len(blob):\n",
    "                    next_cap = False\n",
    "                else:\n",
    "                    next_cap = blob[i+1].isupper()\n",
    "                    next_space = blob[i+1].isspace()    \n",
    "\n",
    "\n",
    "                if i-1 <0:\n",
    "                    prev_lower = False\n",
    "                else:\n",
    "                    prev_lower = blob[i-1].islower()\n",
    "\n",
    "                if (next_cap and prev_lower) or next_space : \n",
    "                    \n",
    "                    #mark \"period\" here as this is likely real end of sentence.\n",
    "                    bexp=bexp + ' <p> '\n",
    "                else:\n",
    "                    #if this is not end of paragraph, then \n",
    "                    #bexp = bexp + char\n",
    "                    pass\n",
    "            else :\n",
    "                bexp = bexp + ' '\n",
    "        elif char.isspace() :\n",
    "            if i-1 >=0 :\n",
    "                if blob[i-1].isspace() :\n",
    "                    pass\n",
    "                else :\n",
    "                    bexp = bexp + char.lower()\n",
    "        elif char == '\\n':\n",
    "            pass\n",
    "\n",
    "        else :\n",
    "            if char.isnumeric() : \n",
    "                bexp = bexp + 'N'\n",
    "            else :\n",
    "                bexp = bexp + char.lower()\n",
    "    \n",
    "    return(bexp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYTData:\n",
    "    __author__ = \"Alan Tan\"\n",
    "    __copyright__ = \"Copyright 2018, Alan Tan\"\n",
    "    __credits__ = [\"Alan Tan\"]\n",
    "    __license__ = \"Apache\"\n",
    "    __version__ = \"2.0\"\n",
    "    __updatedby__ = \"Alan Tan, 2018\"\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        self.source_raw=[]\n",
    "        self.target_raw=[]\n",
    "        self.source_token_ids=[]\n",
    "        self.target_token_ids=[]\n",
    "        self.vocab_to_int={'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 , '<p>': 4}\n",
    "        self.int_to_vocab={0 : '<PAD>', 1: '<EOS>', 2: '<UNK>', 3: '<GO>', 4 : '<p>'}\n",
    "\n",
    "        \n",
    "    def load_from_file(self, input_file_path='nyt_dataset/abstract_nyt_structured_data.csv'):\n",
    "        #read file into Pandas Dataframe.\n",
    "        #each line is one record, it contains \"original file name(including y/m/d)\",\"TITLE\",\"ABSTRACT\",\"LEAD_PARAGRAPH\",\"FULL_TEXT\"\n",
    "        nyt_data = pd.read_csv(input_file_path, \n",
    "                               delimiter=',', index_col=0, header=None, quotechar='\"', quoting=1, \n",
    "                               skipinitialspace=True, engine='c')\n",
    "\n",
    "\n",
    "        #We will use LEAD_PARAGRAPH as source for now\n",
    "        #And ABSTRACT as target. \n",
    "        # Remove punctuation, change digits to NNN, and to lower case.     \n",
    "        self.source_raw=nyt_data[3]\n",
    "        self.target_raw=nyt_data[2]\n",
    "        print(\"Read\",len(self.source_raw),\"records into source text.\")\n",
    "        print(\"Read\",len(self.target_raw),\"records into target text.\")\n",
    "        \n",
    "    def preprocess(self, max_sentence_length=np.inf):\n",
    "        # make a list of unique words\n",
    "        CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "        self.source_token_ids = []\n",
    "        self.target_token_ids = []\n",
    "        \n",
    "        pool = Pool()\n",
    "        source_text = pool.map(sent_tokenize_rm_punct, self.source_raw)\n",
    "        pool.close()\n",
    "        pool.join()        \n",
    "        # serial mode, no parallel multiprocessing .# source_text= sum([sent_tokenize_rm_punct(sentlst) for sentlst in self.source_raw],[])\n",
    "        print(\"source_text length\" , len(source_text))\n",
    "        \n",
    "        pool = Pool()\n",
    "        target_text = pool.map(sent_tokenize_rm_punct, self.target_raw)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        #target_text= sum([sent_tokenize_rm_punct(sentlst) for sentlst in self.target_raw],[])\n",
    "        print(\"target_text length\", len(target_text)) \n",
    "\n",
    "        \n",
    "        source_short_text,target_short_text=zip(*([(st, tt) for st, tt, in zip(source_text, target_text) if len(st.split())<=max_sentence_length*2 and len(tt.split())<=max_sentence_length]))\n",
    "        source_short_length = [len(tt.split()) for tt in source_short_text]\n",
    "        target_short_length = [len(tt.split()) for tt in target_short_text]\n",
    "        print(\"only\", len(source_short_text), \"source sentences are less than\",max(source_short_length), \"long.\")\n",
    "        print(\"only\", len(target_short_text), \"target sentences are less than\",max(target_short_length), \"long.\")\n",
    "        \n",
    "        \n",
    "        #This is a summary model, so we create *one* vocab, (unlike translate model, where 2 separate vocab would be needed)\n",
    "        vocab = set(' '.join(source_text).split()).union(' '.join(target_text).split())\n",
    "        print(\"vocab size\", len(vocab))\n",
    "\n",
    "        # initially vocab starts with the special tokens, and we build all words into the dict vocab_to_int\n",
    "        self.vocab_to_int = copy.copy(CODES)\n",
    "        for w_i, w in enumerate(vocab, len(CODES)):\n",
    "            self.vocab_to_int[w] = w_i\n",
    "            \n",
    "        # create reverse lookup dict(from index to word)\n",
    "        self.int_to_vocab = {w_i: w for w, w_i in self.vocab_to_int.items()}\n",
    "\n",
    "        print(\"vocab dict built:\",len(self.vocab_to_int))\n",
    "        \n",
    "        # create token_ids which are intger indexes of words in our vocab (after tokenization and lower case)\n",
    "        for source_sentence in source_short_text :\n",
    "            source_token_id = [self.vocab_to_int[s] for s in source_sentence.split()]\n",
    "            self.source_token_ids.append(source_token_id)\n",
    "        print(\"source_token_ids size:\",len(self.source_token_ids))\n",
    "            \n",
    "        for target_sentence in target_short_text :\n",
    "            target_token_id = [self.vocab_to_int[s] for s in target_sentence.split()]\n",
    "            self.target_token_ids.append(target_token_id)        \n",
    "        print(\"target_token_ids size:\",len(self.target_token_ids))\n",
    "        \n",
    "\n",
    "    def infer_prep(self,infer_sentences, pad=True):\n",
    "        sentences_int=[]\n",
    "        max_sentence_length=0\n",
    "        for sentence in [sent_tokenize_rm_punct(sent) for sent in infer_sentences] :\n",
    "            \n",
    "            sentence_int=[]\n",
    "            for word in sentence.split() :\n",
    "                if word in self.vocab_to_int :\n",
    "                    sentence_int.append(self.vocab_to_int[word])\n",
    "                else :\n",
    "                    sentence_int.append(self.vocab_to_int['<UNK>'])\n",
    "            sentences_int.append(sentence_int)\n",
    "            max_sentence_length=max(max_sentence_length,len(sentence_int))\n",
    "            \n",
    "        if pad :\n",
    "\n",
    "            \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "            output_ids = [sentence_int + [self.vocab_to_int['<PAD>']] * (max_sentence_length - len(sentence_int)) for sentence_int in sentences_int]\n",
    "        else :\n",
    "            output_ids = sentences_int\n",
    "        return output_ids\n",
    "\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "        for batch_i in range(0, len(self.source_token_ids)//batch_size):\n",
    "            start_i = batch_i * batch_size\n",
    "\n",
    "            # Slice the right amount for the batch\n",
    "            sources_batch = self.source_token_ids[start_i:start_i + batch_size]\n",
    "            targets_batch = self.target_token_ids[start_i:start_i + batch_size]\n",
    "\n",
    "            # Pad\n",
    "            max_sentence = max([len(sentence) for sentence in sources_batch])\n",
    "            pad_sources_batch = np.array([sentence + [self.vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sources_batch])\n",
    "            max_sentence = max([len(sentence) for sentence in targets_batch])\n",
    "            pad_targets_batch = np.array([sentence + [self.vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in targets_batch])\n",
    "\n",
    "            # Need the lengths for the _lengths parameters\n",
    "            pad_targets_lengths = []\n",
    "            for target in pad_targets_batch:\n",
    "                pad_targets_lengths.append(len(target))\n",
    "\n",
    "            pad_sources_lengths = []\n",
    "            for source in pad_sources_batch:\n",
    "                pad_sources_lengths.append(len(source))\n",
    "\n",
    "            yield pad_sources_batch, pad_targets_batch, pad_sources_lengths, pad_targets_lengths\n",
    "\n",
    "\n",
    "        \n",
    "    def save_preprocess(self, save_file_name='preprocess.p'):    \n",
    "        # Save data for later use\n",
    "        pickle.dump(self, open(save_file_name, 'wb'))\n",
    "        \n",
    "        print(\"Saved (\", len(self.source_token_ids),  len(self.target_token_ids) , \") source , target tokens\")\n",
    "        print(\"   ...and (\",len(self.vocab_to_int),\") voc-int dictionary\")\n",
    "        print(\"   ...and (\",len(self.int_to_vocab),\") int-voc dictionary\")\n",
    "    \n",
    "    def load_preprocess(self, load_file_name='preprocess.p'):\n",
    "        with open(load_file_name, mode='rb') as in_file:\n",
    "            \n",
    "            self=pickle.load(in_file)        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 332197\n",
      "Tokenized articles: 98370\n",
      "Longest raw source Sentence: 1695\n",
      "Longest preprocessed source Sentence: 64\n",
      "Longest raw target Sentence: 782\n",
      "Longest preprocessed target Sentence: 32\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from_scratch=False\n",
    "    save_file=work_dir+'short_preprocess.p'\n",
    "    NYTDATA=NYTData()\n",
    "    \n",
    "    if from_scratch :\n",
    "        NYTDATA.load_from_file('nyt_dataset/short_abstract_nyt_structured_data.csv')\n",
    "        NYTDATA.preprocess(max_sentence_length=50)\n",
    "    \n",
    "        with open(save_file, 'wb') as f:\n",
    "            pickle.dump(NYTDATA, f)\n",
    "    else : #to load back\n",
    "        with open(save_file, 'rb') as f:\n",
    "            NYTDATA = pickle.load(f)\n",
    "    \n",
    "    print('Vocab Size:', len(NYTDATA.vocab_to_int))\n",
    "    print('Tokenized articles:', len(NYTDATA.source_token_ids))\n",
    "    print('Longest raw source Sentence:', max([len(sent) for sent in NYTDATA.source_raw]))\n",
    "    print('Longest preprocessed source Sentence:', max([len(sent) for sent in NYTDATA.source_token_ids]))\n",
    "    print('Longest raw target Sentence:', max([len(sent) for sent in NYTDATA.target_raw]))\n",
    "    print('Longest preprocessed target Sentence:', max([len(sent) for sent in NYTDATA.target_token_ids]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routines to build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a training process in decoding layer \n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "\n",
    "    #decoder_initial_state = dec_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "    #decoder_initial_state = encoder_state\n",
    "\n",
    "    #print(\"decoder_init_state is type:\", type(decoder_initial_state))\n",
    "    #print(decoder_initial_state)\n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              initial_state=encoder_state, \n",
    "                                              output_layer=output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a inference process in decoding layer \n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    \n",
    "    #decoder_initial_state = encoder_state\n",
    "    #decoder_initial_state = dec_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "\n",
    "    \n",
    "    #dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "    #                                         output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              initial_state=encoder_state, \n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size, enc_outputs):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    \n",
    "    \n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    #print(dec_embed_input)\n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(cells, output_keep_prob=keep_prob)\n",
    "    if use_attention :\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units=batch_size, memory=enc_outputs) \n",
    "                #,                memory_sequence_length=150)\n",
    "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                dec_cell, attention_mechanism, attention_layer_size=150)\n",
    "        dec_initial_state = dec_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(dec_initial_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(dec_initial_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, (enc_states) = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    #print(type(enc_states))\n",
    "    #print(len(enc_states))\n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                               num_layers,\n",
    "                                               target_vocab_to_int,\n",
    "                                               target_vocab_size,\n",
    "                                               batch_size,\n",
    "                                               keep_prob,\n",
    "                                               dec_embedding_size, enc_outputs)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subroutines used in training\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n",
    "        \n",
    "        \n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "\n",
    "#Subrountines used in preparing data for inference. \n",
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph prepared... 2018-12-13 19:55:43.686662\n"
     ]
    }
   ],
   "source": [
    "#set intervals for peek into the training progress: \n",
    "peek_interval=datetime.timedelta(minutes=10)\n",
    "#Please note peek  may not be on exact above period, because we need to allow each batch to finish. \n",
    "#So the batch that finishes 10-min (the period set above) would be where we take a peek\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "#Add no_peek threshold so system will not peek too frequently on very slow model/systems\n",
    "only_peek_after = int(len(NYTDATA.source_token_ids)/batch_size/100)\n",
    "\n",
    "rnn_size = 200\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 300\n",
    "decoding_embedding_size = encoding_embedding_size\n",
    "\n",
    "use_attention=True\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.95\n",
    "save_path = \"working_dir/\"\n",
    "\n",
    "\n",
    "#NYTDATA.load_preprocess()\n",
    "\n",
    "max_target_sentence_length = max([len(sentence) for sentence in NYTDATA.source_token_ids])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    \n",
    "    #lr, keep_prob = hyperparam_inputs()\n",
    "    lr = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "  \n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(NYTDATA.vocab_to_int),\n",
    "                                                   len(NYTDATA.vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   NYTDATA.vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        #capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        capped_gradients = [(tf.clip_by_norm(grad, 50), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph prepared...\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data to training and validation sets\n",
    "print(\"Splitting train/valid...\", datetime.datetime.now())\n",
    "\n",
    "train_source = NYTDATA.source_token_ids[batch_size:]\n",
    "train_target = NYTDATA.target_token_ids[batch_size:]\n",
    "valid_source = NYTDATA.source_token_ids[:batch_size]\n",
    "valid_target = NYTDATA.target_token_ids[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             NYTDATA.vocab_to_int['<PAD>'],\n",
    "                                                                                                             NYTDATA.vocab_to_int['<PAD>']))   \n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"start training...\", datetime.datetime.now())\n",
    "    last_peek_time=datetime.datetime.now()\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "            NYTDATA.get_batches(batch_size)):\n",
    "#                get_batches(train_source, train_target, batch_size,\n",
    "#                            NYTDATA.vocab_to_int['<PAD>'],\n",
    "#                            NYTDATA.vocab_to_int['<PAD>'])):\n",
    "            print(\".\",end='')\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            #check time, and peek into progress if time passed exceeds interval set\n",
    "            current_time=datetime.datetime.now()\n",
    "            if (current_time - last_peek_time) > peek_interval :\n",
    "                last_peek_time=current_time\n",
    "            #if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 0.95})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 0.95})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "                \n",
    "                print(datetime.datetime.now())\n",
    "                print('\\nEpoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(NYTDATA.source_token_ids) // batch_size, train_acc, valid_acc, loss))\n",
    "                print('  source: {}'.format([NYTDATA.int_to_vocab[i] for i in valid_sources_batch[int((1+epoch_i)*(batch_i))%batch_size]]))\n",
    "                print('  target: {}'.format([NYTDATA.int_to_vocab[i] for i in valid_target_batch[int((1+epoch_i)*(batch_i))%batch_size]]))\n",
    "                print('  summary: {}'.format([NYTDATA.int_to_vocab[i] for i in batch_valid_logits[int((1+epoch_i)*(batch_i))%batch_size]]))\n",
    "\n",
    "        # Save checkpoint of training\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save_path+'ckpt', global_step=epoch_i)\n",
    "        print('\\nModel training checkpoing for epoch_{} is saved'.format(epoch_i))                   \n",
    "    \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('working_dir/100words_or_less_preprocess.p', 'rb') as f:\n",
    "#            NYTDATA = pickle.load(f)\n",
    "#batch_size=100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#test inference_logits\n",
    "   \n",
    "test_sentences_raw = [' '.join(shortern.split()) for shortern in NYTDATA.source_raw[0:0+batch_size]]\n",
    "test_target_raw = [' '.join(shortern.split()) for shortern in NYTDATA.target_raw[0:0+batch_size]]\n",
    "\n",
    "print(len(test_sentences_raw))\n",
    "\n",
    "test_target_tokenized = [sent_tokenize_rm_punct(' '.join(shortern.split())) for shortern in NYTDATA.source_raw[0:0+batch_size]]\n",
    "print(len(test_target_tokenized))\n",
    "\n",
    "test_sentences = NYTDATA.infer_prep(test_sentences_raw, pad=True)[0:batch_size]\n",
    "print(len(test_sentences))\n",
    "test_sentence_length = [len(ts) for ts in test_sentences]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from working_dir/ckpt-7\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    checkpoint_file=save_path+'ckpt-7'\n",
    "    loader = tf.train.import_meta_graph(checkpoint_file + '.meta')\n",
    "    loader.restore(sess, checkpoint_file)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    inference_logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    \n",
    "    translate_logits = sess.run(inference_logits, {input_data: test_sentences,\n",
    "                                         target_sequence_length: test_sentence_length,\n",
    "                                         keep_prob: 0.95})\n",
    "    for ii in range(len(test_sentences)) :\n",
    "        print('\\nRaw: {}'.format(test_sentences_raw[ii]))\n",
    "        #print('  Word Ids:      {}'.format([i for i in test_sentences[ii]]))\n",
    "        print('\\nInput: {}'.format([NYTDATA.int_to_vocab[i] for i in test_sentences[ii] if i > 0]))\n",
    "\n",
    "        #print('\\nOutput')\n",
    "        #print('  Word Ids:      {}'.format([i for i in translate_logits[ii]]))\n",
    "        print('\\nActual Abstract: {}'.format(test_target_raw[ii]))\n",
    "        print('  \\nOutput: {}\\n'.format(\" \".join([NYTDATA.int_to_vocab[i] for i in translate_logits[ii] if i>0])))\n",
    "    #finished test infer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE_1:7.389135000905854\n",
      "ROUGE_2:0.7314546862200986\n",
      "ROUGE_l:2.9458417751032235\n"
     ]
    }
   ],
   "source": [
    "import rouge\n",
    "hypotheses=[]\n",
    "references=test_target_tokenized\n",
    "for ii in range(len(test_sentences)):\n",
    "    hypotheses.append(\" \".join([NYTDATA.int_to_vocab[i] for i in translate_logits[ii] if i > 0 ]))\n",
    "\n",
    "rouge_score_map = rouge.rouge(hypotheses, references)\n",
    "print(\"ROUGE_1:{}\".format(100 * rouge_score_map[\"rouge_1/f_score\"]))\n",
    "print(\"ROUGE_2:{}\".format(100 * rouge_score_map[\"rouge_2/f_score\"]))\n",
    "print(\"ROUGE_l:{}\".format(100 * rouge_score_map[\"rouge_l/f_score\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
